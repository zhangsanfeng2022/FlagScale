defaults:
  - train: train_llava1.5_7b.yaml
  - _self_

experiment:
  exp_name: llava1.5
  exp_dir: ./outputs_llava1.5
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/train_llava.py
  runner:
    backend: torchrun
    nnodes: 4 
    nproc_per_node: 8
    hostfile: <xxx>
  envs:
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7 
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    NVTE_APPLY_QK_LAYER_SCALING: 0
    NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
    ALLGATHER_ASYNC: false
    ALLREDUCE_ASYNC: false
    ALLREDUCE_FUSION: 0
    BKCL_CCIX_BUFFER_GM: 1
    BKCL_CCIX_RING: 1
    BKCL_ENABLE_XDR: 1
    BKCL_FLAT_RING: 1
    BKCL_KL3_TURBO_MODE: 1
    BKCL_RDMA_FORCE_TREE: 1
    BKCL_RDMA_NICS: ens11np0,ens11np0,ens13np0,ens13np0,ens15np0,ens15np0,ens17np0,ens17np0
    BKCL_RDMA_PROXY_DISABLE: 1
    BKCL_RING_BUFFER_GM: 1
    BKCL_TIMEOUT: 360000
    BKCL_TRANS_UNSUPPORTED_DATATYPE: 1
    BKCL_TREE_THRESHOLD: 1
    BKCL_XLINK_C2C: 1
    BKCL_XLINK_D2D: 0
    BKCL_XLINK_ETH: 0
    CUDART_DUMMY_REGISTER: 1
    FAST_SWIGLU_ENABLE: 1
    USE_FAST_BF16_FC: true
    USE_L3: 1
    XDNN_USE_FAST_SWISH: true
    XPU_ZEBU_MODE: 1

action: run 

hydra: 
  run:
    dir: ${experiment.exp_dir}/hydra
